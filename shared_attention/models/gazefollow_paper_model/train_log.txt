GazeFollow
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
head_image (InputLayer)         (None, 224, 224, 3)  0
__________________________________________________________________________________________________
vgg16 (Model)                   (None, 7, 7, 512)    14714688    head_image[0][0]
__________________________________________________________________________________________________
gaze_flatten1 (Flatten)         (None, 25088)        0           vgg16[1][0]
__________________________________________________________________________________________________
gaze_dense1 (Dense)             (None, 128)          3211264     gaze_flatten1[0][0]
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 128)          512         gaze_dense1[0][0]
__________________________________________________________________________________________________
head_location (InputLayer)      (None, 2, 1, 1)      0
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128)          0           batch_normalization_2[0][0]
__________________________________________________________________________________________________
head_location_flattened (Flatte (None, 2)            0           head_location[0][0]
__________________________________________________________________________________________________
merged_gaze_features (Concatena (None, 130)          0           activation_2[0][0]
                                                                 head_location_flattened[0][0]
__________________________________________________________________________________________________
gaze_dense2 (Dense)             (None, 300)          39000       merged_gaze_features[0][0]
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 300)          1200        gaze_dense2[0][0]
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 300)          0           batch_normalization_3[0][0]
__________________________________________________________________________________________________
gaze_dense3 (Dense)             (None, 250)          75000       activation_3[0][0]
__________________________________________________________________________________________________
input_image (InputLayer)        (None, 224, 224, 3)  0
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 250)          1000        gaze_dense3[0][0]
__________________________________________________________________________________________________
vgg16-hybrid1365 (Model)        (None, 14, 14, 512)  14714688    input_image[0][0]
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 250)          0           batch_normalization_4[0][0]
__________________________________________________________________________________________________
saliency_conv1 (Conv2D)         (None, 14, 14, 1)    512         vgg16-hybrid1365[1][0]
__________________________________________________________________________________________________
gaze_dense4 (Dense)             (None, 196)          49000       activation_4[0][0]
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 14, 14, 1)    4           saliency_conv1[0][0]
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 196)          784         gaze_dense4[0][0]
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 14, 14, 1)    0           batch_normalization_1[0][0]
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 196)          0           batch_normalization_5[0][0]
__________________________________________________________________________________________________
saliency_map_flattened (Flatten (None, 196)          0           activation_1[0][0]
__________________________________________________________________________________________________
combined_heatmaps (Multiply)    (None, 196)          0           activation_5[0][0]
                                                                 saliency_map_flattened[0][0]
__________________________________________________________________________________________________
output_location (Dense)         (None, 25)           4925        combined_heatmaps[0][0]
==================================================================================================
Total params: 32,812,577
Trainable params: 17,540,299
Non-trainable params: 15,272,278
__________________________________________________________________________________________________
None
Tensor("output_location/Softmax:0", shape=(?, 25), dtype=float32)
Epoch 1/100
1247/1247 [============================>.] - ETA: 0s - loss: 2.3329 - acc: 0.3601Shuffling data
1248/1247 [==============================] - 160s 128ms/step - loss: 2.3322 - acc: 0.3604 - val_loss: 3.1868 - val_acc: 0.1769
1000 . 2000 .
Epoch 00001: loss improved from inf to 2.33216, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 2/100
1248/1247 [==============================] - 156s 125ms/step - loss: 1.6633 - acc: 0.5808 - val_loss: 3.6414 - val_acc: 0.1753

Epoch 00002: loss improved from 2.33216 to 1.66329, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 3/100
1248/1247 [==============================] - 156s 125ms/step - loss: 1.3392 - acc: 0.6764 - val_loss: 3.7990 - val_acc: 0.1566

Epoch 00003: loss improved from 1.66329 to 1.33923, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 4/100
1248/1247 [==============================] - 156s 125ms/step - loss: 1.1473 - acc: 0.7397 - val_loss: 3.8904 - val_acc: 0.1749

Epoch 00004: loss improved from 1.33923 to 1.14732, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 5/100
1248/1247 [==============================] - 156s 125ms/step - loss: 1.0207 - acc: 0.7776 - val_loss: 3.8670 - val_acc: 0.1598

Epoch 00005: loss improved from 1.14732 to 1.02075, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 6/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.9200 - acc: 0.8115 - val_loss: 4.1967 - val_acc: 0.1741

Epoch 00006: loss improved from 1.02075 to 0.91995, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 7/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.8632 - acc: 0.8263 - val_loss: 3.9043 - val_acc: 0.1749

Epoch 00007: loss improved from 0.91995 to 0.86317, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 8/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.8091 - acc: 0.8422 - val_loss: 3.9445 - val_acc: 0.1812

Epoch 00008: loss improved from 0.86317 to 0.80910, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 9/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.7886 - acc: 0.8517 - val_loss: 4.0032 - val_acc: 0.1399

Epoch 00009: loss improved from 0.80910 to 0.78863, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 10/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.7485 - acc: 0.8602 - val_loss: 4.0603 - val_acc: 0.1709

Epoch 00010: loss improved from 0.78863 to 0.74847, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 11/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.7043 - acc: 0.8780 - val_loss: 4.4548 - val_acc: 0.1832

Epoch 00011: loss improved from 0.74847 to 0.70429, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 12/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.7010 - acc: 0.8783 - val_loss: 4.4117 - val_acc: 0.1757

Epoch 00012: loss improved from 0.70429 to 0.70099, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 13/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.6660 - acc: 0.8879 - val_loss: 4.3960 - val_acc: 0.1598

Epoch 00013: loss improved from 0.70099 to 0.66602, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 14/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.6540 - acc: 0.8900 - val_loss: 4.0169 - val_acc: 0.1538

Epoch 00014: loss improved from 0.66602 to 0.65399, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 15/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.6266 - acc: 0.8952 - val_loss: 3.9350 - val_acc: 0.1638

Epoch 00015: loss improved from 0.65399 to 0.62657, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 16/100
1248/1247 [==============================] - 156s 125ms/step - loss: 0.6131 - acc: 0.9002 - val_loss: 4.4385 - val_acc: 0.1705

Epoch 00016: loss improved from 0.62657 to 0.61307, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 17/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5926 - acc: 0.9087 - val_loss: 4.7875 - val_acc: 0.1932

Epoch 00017: loss improved from 0.61307 to 0.59258, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 18/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5882 - acc: 0.9082 - val_loss: 4.6889 - val_acc: 0.1832

Epoch 00018: loss improved from 0.59258 to 0.58822, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 19/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5818 - acc: 0.9117 - val_loss: 3.8823 - val_acc: 0.1645

Epoch 00019: loss improved from 0.58822 to 0.58185, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 20/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5715 - acc: 0.9138 - val_loss: 4.4302 - val_acc: 0.1626

Epoch 00020: loss improved from 0.58185 to 0.57146, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 21/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.5538 - acc: 0.9181 - val_loss: 5.1715 - val_acc: 0.1713

Epoch 00021: loss improved from 0.57146 to 0.55381, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 22/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5579 - acc: 0.9161 - val_loss: 4.4294 - val_acc: 0.1622

Epoch 00022: loss did not improve from 0.55381
Epoch 23/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5381 - acc: 0.9214 - val_loss: 4.9611 - val_acc: 0.1832

Epoch 00023: loss improved from 0.55381 to 0.53810, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 24/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.5424 - acc: 0.9185 - val_loss: 4.6452 - val_acc: 0.1800

Epoch 00024: loss did not improve from 0.53810
Epoch 25/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.5228 - acc: 0.9215 - val_loss: 4.6850 - val_acc: 0.1836

Epoch 00025: loss improved from 0.53810 to 0.52279, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 26/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5095 - acc: 0.9290 - val_loss: 5.3153 - val_acc: 0.1653

Epoch 00026: loss improved from 0.52279 to 0.50951, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 27/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.5076 - acc: 0.9280 - val_loss: 4.9852 - val_acc: 0.1800

Epoch 00027: loss improved from 0.50951 to 0.50763, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 28/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5011 - acc: 0.9318 - val_loss: 4.1253 - val_acc: 0.1753

Epoch 00028: loss improved from 0.50763 to 0.50112, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 29/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4900 - acc: 0.9323 - val_loss: 4.8206 - val_acc: 0.1689

Epoch 00029: loss improved from 0.50112 to 0.49005, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 30/100
1248/1247 [==============================] - 157s 125ms/step - loss: 0.5005 - acc: 0.9296 - val_loss: 4.9184 - val_acc: 0.1765

Epoch 00030: loss did not improve from 0.49005
Epoch 31/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4866 - acc: 0.9351 - val_loss: 4.9857 - val_acc: 0.1673

Epoch 00031: loss improved from 0.49005 to 0.48661, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 32/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4917 - acc: 0.9345 - val_loss: 4.9726 - val_acc: 0.1638

Epoch 00032: loss did not improve from 0.48661
Epoch 33/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4691 - acc: 0.9395 - val_loss: 4.8644 - val_acc: 0.1749

Epoch 00033: loss improved from 0.48661 to 0.46905, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 34/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4757 - acc: 0.9346 - val_loss: 4.9396 - val_acc: 0.2047

Epoch 00034: loss did not improve from 0.46905
Epoch 35/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4605 - acc: 0.9427 - val_loss: 4.8655 - val_acc: 0.1944

Epoch 00035: loss improved from 0.46905 to 0.46053, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 36/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4623 - acc: 0.9409 - val_loss: 5.0384 - val_acc: 0.1681

Epoch 00036: loss did not improve from 0.46053
Epoch 37/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4545 - acc: 0.9415 - val_loss: 5.3938 - val_acc: 0.1832

Epoch 00037: loss improved from 0.46053 to 0.45450, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 38/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4547 - acc: 0.9423 - val_loss: 4.9140 - val_acc: 0.1900

Epoch 00038: loss did not improve from 0.45450
Epoch 39/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4464 - acc: 0.9434 - val_loss: 4.7416 - val_acc: 0.1681

Epoch 00039: loss improved from 0.45450 to 0.44644, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 40/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4477 - acc: 0.9434 - val_loss: 4.7557 - val_acc: 0.1538

Epoch 00040: loss did not improve from 0.44644
Epoch 41/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4389 - acc: 0.9452 - val_loss: 5.3663 - val_acc: 0.1741

Epoch 00041: loss improved from 0.44644 to 0.43886, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 42/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4432 - acc: 0.9439 - val_loss: 4.6063 - val_acc: 0.1912

Epoch 00042: loss did not improve from 0.43886
Epoch 43/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4332 - acc: 0.9469 - val_loss: 5.0835 - val_acc: 0.1622

Epoch 00043: loss improved from 0.43886 to 0.43324, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 44/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4325 - acc: 0.9451 - val_loss: 4.6173 - val_acc: 0.1797

Epoch 00044: loss improved from 0.43324 to 0.43255, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 45/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4166 - acc: 0.9495 - val_loss: 6.0017 - val_acc: 0.2015

Epoch 00045: loss improved from 0.43255 to 0.41665, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 46/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4236 - acc: 0.9488 - val_loss: 5.0839 - val_acc: 0.1630

Epoch 00046: loss did not improve from 0.41665
Epoch 47/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4150 - acc: 0.9516 - val_loss: 5.4995 - val_acc: 0.1963

Epoch 00047: loss improved from 0.41665 to 0.41497, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 48/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4167 - acc: 0.9494 - val_loss: 5.9992 - val_acc: 0.1955

Epoch 00048: loss did not improve from 0.41497
Epoch 49/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4110 - acc: 0.9526 - val_loss: 4.9729 - val_acc: 0.2011

Epoch 00049: loss improved from 0.41497 to 0.41102, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 50/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4108 - acc: 0.9521 - val_loss: 5.6067 - val_acc: 0.1800

Epoch 00050: loss improved from 0.41102 to 0.41084, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 51/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4074 - acc: 0.9533 - val_loss: 5.4140 - val_acc: 0.1991

Epoch 00051: loss improved from 0.41084 to 0.40743, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 52/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4040 - acc: 0.9533 - val_loss: 5.3177 - val_acc: 0.1904

Epoch 00052: loss improved from 0.40743 to 0.40402, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 53/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3999 - acc: 0.9545 - val_loss: 5.0566 - val_acc: 0.1816

Epoch 00053: loss improved from 0.40402 to 0.39985, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 54/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.4006 - acc: 0.9530 - val_loss: 5.2998 - val_acc: 0.1665

Epoch 00054: loss did not improve from 0.39985
Epoch 55/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3952 - acc: 0.9553 - val_loss: 5.4375 - val_acc: 0.2071

Epoch 00055: loss improved from 0.39985 to 0.39524, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 56/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3910 - acc: 0.9568 - val_loss: 5.2467 - val_acc: 0.2079

Epoch 00056: loss improved from 0.39524 to 0.39105, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 57/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3938 - acc: 0.9554 - val_loss: 5.8072 - val_acc: 0.1868

Epoch 00057: loss did not improve from 0.39105
Epoch 58/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3885 - acc: 0.9551 - val_loss: 5.3826 - val_acc: 0.1844

Epoch 00058: loss improved from 0.39105 to 0.38851, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 59/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3864 - acc: 0.9568 - val_loss: 4.9287 - val_acc: 0.1661

Epoch 00059: loss improved from 0.38851 to 0.38635, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 60/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3848 - acc: 0.9572 - val_loss: 5.3291 - val_acc: 0.2150

Epoch 00060: loss improved from 0.38635 to 0.38484, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 61/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3917 - acc: 0.9555 - val_loss: 6.0483 - val_acc: 0.1498

Epoch 00061: loss did not improve from 0.38484
Epoch 62/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3802 - acc: 0.9582 - val_loss: 5.6472 - val_acc: 0.1681

Epoch 00062: loss improved from 0.38484 to 0.38019, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 63/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3699 - acc: 0.9613 - val_loss: 6.3670 - val_acc: 0.1836

Epoch 00063: loss improved from 0.38019 to 0.36995, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 64/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3797 - acc: 0.9573 - val_loss: 5.8556 - val_acc: 0.1741

Epoch 00064: loss did not improve from 0.36995
Epoch 65/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3705 - acc: 0.9610 - val_loss: 5.7805 - val_acc: 0.1657

Epoch 00065: loss did not improve from 0.36995
Epoch 66/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3667 - acc: 0.9615 - val_loss: 5.4051 - val_acc: 0.1908

Epoch 00066: loss improved from 0.36995 to 0.36670, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 67/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3739 - acc: 0.9591 - val_loss: 6.2239 - val_acc: 0.1733

Epoch 00067: loss did not improve from 0.36670
Epoch 68/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3674 - acc: 0.9614 - val_loss: 6.5635 - val_acc: 0.1761

Epoch 00068: loss did not improve from 0.36670
Epoch 69/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3723 - acc: 0.9609 - val_loss: 4.4898 - val_acc: 0.1578

Epoch 00069: loss did not improve from 0.36670
Epoch 70/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3697 - acc: 0.9606 - val_loss: 6.0367 - val_acc: 0.1797

Epoch 00070: loss did not improve from 0.36670
Epoch 71/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3636 - acc: 0.9626 - val_loss: 6.2146 - val_acc: 0.1916

Epoch 00071: loss improved from 0.36670 to 0.36364, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 72/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3613 - acc: 0.9622 - val_loss: 5.7350 - val_acc: 0.1705

Epoch 00072: loss improved from 0.36364 to 0.36130, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 73/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3557 - acc: 0.9634 - val_loss: 5.5916 - val_acc: 0.1626

Epoch 00073: loss improved from 0.36130 to 0.35572, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 74/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3585 - acc: 0.9617 - val_loss: 6.0199 - val_acc: 0.1872

Epoch 00074: loss did not improve from 0.35572
Epoch 75/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3570 - acc: 0.9624 - val_loss: 5.9330 - val_acc: 0.1804

Epoch 00075: loss did not improve from 0.35572
Epoch 76/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3552 - acc: 0.9632 - val_loss: 5.6218 - val_acc: 0.1999

Epoch 00076: loss improved from 0.35572 to 0.35525, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 77/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3592 - acc: 0.9625 - val_loss: 5.8385 - val_acc: 0.1804

Epoch 00077: loss did not improve from 0.35525
Epoch 78/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3467 - acc: 0.9661 - val_loss: 5.6717 - val_acc: 0.1832

Epoch 00078: loss improved from 0.35525 to 0.34675, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 79/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3515 - acc: 0.9640 - val_loss: 6.0462 - val_acc: 0.1872

Epoch 00079: loss did not improve from 0.34675
Epoch 80/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3510 - acc: 0.9645 - val_loss: 6.1122 - val_acc: 0.1685

Epoch 00080: loss did not improve from 0.34675
Epoch 81/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3508 - acc: 0.9632 - val_loss: 5.5852 - val_acc: 0.1697

Epoch 00081: loss did not improve from 0.34675
Epoch 82/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3421 - acc: 0.9657 - val_loss: 6.1378 - val_acc: 0.1606

Epoch 00082: loss improved from 0.34675 to 0.34211, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 83/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3372 - acc: 0.9671 - val_loss: 6.8252 - val_acc: 0.1800

Epoch 00083: loss improved from 0.34211 to 0.33715, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 84/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3499 - acc: 0.9634 - val_loss: 5.1634 - val_acc: 0.1955

Epoch 00084: loss did not improve from 0.33715
Epoch 85/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3364 - acc: 0.9679 - val_loss: 6.0931 - val_acc: 0.1546

Epoch 00085: loss improved from 0.33715 to 0.33639, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 86/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3381 - acc: 0.9667 - val_loss: 5.8970 - val_acc: 0.1729

Epoch 00086: loss did not improve from 0.33639
Epoch 87/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3391 - acc: 0.9661 - val_loss: 5.7181 - val_acc: 0.1717

Epoch 00087: loss did not improve from 0.33639
Epoch 88/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3339 - acc: 0.9678 - val_loss: 6.5331 - val_acc: 0.1908

Epoch 00088: loss improved from 0.33639 to 0.33387, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 89/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3366 - acc: 0.9665 - val_loss: 6.4696 - val_acc: 0.1657

Epoch 00089: loss did not improve from 0.33387
Epoch 90/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3368 - acc: 0.9671 - val_loss: 6.0093 - val_acc: 0.1800

Epoch 00090: loss did not improve from 0.33387
Epoch 91/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3358 - acc: 0.9685 - val_loss: 6.0439 - val_acc: 0.1590

Epoch 00091: loss did not improve from 0.33387
Epoch 92/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3299 - acc: 0.9706 - val_loss: 5.7727 - val_acc: 0.1840

Epoch 00092: loss improved from 0.33387 to 0.32995, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 93/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3223 - acc: 0.9718 - val_loss: 5.7870 - val_acc: 0.1538

Epoch 00093: loss improved from 0.32995 to 0.32234, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 94/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3319 - acc: 0.9684 - val_loss: 6.0629 - val_acc: 0.1904

Epoch 00094: loss did not improve from 0.32234
Epoch 95/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3265 - acc: 0.9699 - val_loss: 6.1707 - val_acc: 0.1908

Epoch 00095: loss did not improve from 0.32234
Epoch 96/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3213 - acc: 0.9708 - val_loss: 6.0632 - val_acc: 0.1856

Epoch 00096: loss improved from 0.32234 to 0.32134, saving model to data/checkpoints/best_00-44-19.hdf5
Epoch 97/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3314 - acc: 0.9678 - val_loss: 6.2794 - val_acc: 0.1832

Epoch 00097: loss did not improve from 0.32134
Epoch 98/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3221 - acc: 0.9707 - val_loss: 6.7235 - val_acc: 0.1753

Epoch 00098: loss did not improve from 0.32134
Epoch 99/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3223 - acc: 0.9709 - val_loss: 5.9936 - val_acc: 0.2079

Epoch 00099: loss did not improve from 0.32134
Epoch 100/100
1248/1247 [==============================] - 157s 126ms/step - loss: 0.3289 - acc: 0.9685 - val_loss: 6.1081 - val_acc: 0.1793

Epoch 00100: loss did not improve from 0.32134
Training history:
('acc', [0.3603766025641026, 0.5807792467948718, 0.6764322916666666, 0.7396834935897436, 0.7776442307692307, 0.8115484775641025, 0.8262720352564102, 0.8422475961538461, 0.8517127403846154, 0.860176282051282, 0.8780048076923077, 0.8782552083333334, 0.887870592948718, 0.8900240384615384, 0.8951822916666666, 0.9001903044871795, 0.9087039262820513, 0.908203125, 0.9117087339743589, 0.9138120993589743, 0.9180689102564102, 0.9161157852564102, 0.9214242788461539, 0.9184695512820513, 0.9214743589743589, 0.9289863782051282, 0.9280348557692307, 0.9318409455128205, 0.9322916666666666, 0.9296374198717948, 0.9351462339743589, 0.9345452724358975, 0.939453125, 0.9346454326923077, 0.9426582532051282, 0.9409054487179487, 0.9415064102564102, 0.9423076923076923, 0.943359375, 0.9434094551282052, 0.9452123397435898, 0.9439102564102564, 0.9469150641025641, 0.9450620993589743, 0.9494691506410257, 0.9488181089743589, 0.9515725160256411, 0.9494190705128205, 0.9526241987179487, 0.9521233974358975, 0.9532752403846154, 0.9532752403846154, 0.9545272435897436, 0.9529747596153846, 0.9552784455128205, 0.9568309294871795, 0.9553786057692307, 0.9551282051282052, 0.9568309294871795, 0.9571814903846154, 0.9555288461538461, 0.9582331730769231, 0.9613381410256411, 0.9572816506410257, 0.9610376602564102, 0.9614883814102564, 0.9591346153846154, 0.9613882211538461, 0.9608874198717948, 0.9606370192307693, 0.9625901442307693, 0.9621895032051282, 0.9634415064102564, 0.9616887019230769, 0.9624399038461539, 0.9632411858974359, 0.9624899839743589, 0.9660957532051282, 0.964042467948718, 0.9645432692307693, 0.9631911057692307, 0.9656951121794872, 0.9671474358974359, 0.9633914262820513, 0.967948717948718, 0.9667467948717948, 0.9660957532051282, 0.9678485576923077, 0.9664963942307693, 0.9671474358974359, 0.9685496794871795, 0.9706029647435898, 0.9718048878205128, 0.9683994391025641, 0.969901842948718, 0.9708032852564102, 0.9678485576923077, 0.9706530448717948, 0.9709034455128205, 0.9684995993589743])
('loss', [2.3321576733619738, 1.6632917552995377, 1.339227121657668, 1.1473231509757729, 1.0207492982347806, 0.9199516015031781, 0.8631720080112035, 0.8090960750690638, 0.7886344188681016, 0.7484689313345231, 0.7042861972242976, 0.7009865532939633, 0.6660239695069882, 0.6539867432453693, 0.6265694457464493, 0.6130738883780745, 0.5925839468597983, 0.5882188580595912, 0.5818487246735737, 0.5714604700318514, 0.5538098621062744, 0.557885805861308, 0.5380982894880267, 0.5424397406287682, 0.5227860528259323, 0.5095091655563849, 0.5076283821836114, 0.5011248994284333, 0.49004621080194527, 0.5004792887813005, 0.486613697778338, 0.4917241561059386, 0.46905486283298486, 0.4757132696895263, 0.4605274118292026, 0.462344578300149, 0.45449569050031596, 0.4547381558431647, 0.44643903198914653, 0.4477295048821431, 0.4388569178871619, 0.44324491153924894, 0.4332422317029574, 0.432548873198147, 0.41664903579900664, 0.4235945275674264, 0.4149659033387135, 0.41670336278203207, 0.41102045215666294, 0.4108421345695089, 0.4074323706042308, 0.40401935226355606, 0.3998541696092639, 0.40055793893929476, 0.3952367284263556, 0.391049549389535, 0.39384496906915534, 0.3885094403551939, 0.38635011022098553, 0.3848414651046579, 0.39169513064030653, 0.3801851625769184, 0.369946007640698, 0.3797017768359719, 0.3705449285558783, 0.3667001637319724, 0.37393656401680064, 0.3673636629365576, 0.37229954766539425, 0.3696845884267718, 0.3636392871729838, 0.3613041351334407, 0.3557216029088849, 0.3585129687562585, 0.35699417122090477, 0.3552452127616375, 0.35916139541241604, 0.34674722658326995, 0.35145580178747576, 0.35101368533781707, 0.350838662411731, 0.3421057392962468, 0.33715275961619157, 0.3498503550982628, 0.33639260319372016, 0.33808506416300166, 0.3390940867889768, 0.33387105090495867, 0.3366340792809541, 0.336815512404801, 0.33581572374663293, 0.3299465864323653, 0.3223411816960344, 0.33188832398408497, 0.32653199781018954, 0.3213385267613026, 0.3314208939958077, 0.32214342031436854, 0.32230248677138335, 0.3289479777837793])
('val_acc', [0.17686804451510335, 0.17527821939586646, 0.15659777424483307, 0.17488076311605724, 0.15977742448330684, 0.1740858505564388, 0.17488076311605724, 0.18124006359300476, 0.13990461049284578, 0.17090620031796502, 0.18322734499205087, 0.17567567567567569, 0.15977742448330684, 0.1538155802861685, 0.16375198728139906, 0.1705087440381558, 0.1931637519872814, 0.18322734499205087, 0.16454689984101747, 0.1625596184419714, 0.17130365659777425, 0.16216216216216217, 0.18322734499205087, 0.1800476947535771, 0.1836248012718601, 0.16534181240063592, 0.1800476947535771, 0.17527821939586646, 0.16891891891891891, 0.17647058823529413, 0.16732909379968203, 0.16375198728139906, 0.17488076311605724, 0.2046899841017488, 0.19435612082670906, 0.16812400635930047, 0.18322734499205087, 0.18998410174880764, 0.16812400635930047, 0.1538155802861685, 0.1740858505564388, 0.19117647058823528, 0.16216216216216217, 0.17965023847376788, 0.20151033386327505, 0.16295707472178061, 0.19634340222575516, 0.19554848966613672, 0.20111287758346583, 0.1800476947535771, 0.19912559618441972, 0.19038155802861687, 0.18163751987281399, 0.16653418124006358, 0.20707472178060413, 0.20786963434022257, 0.18680445151033387, 0.18441971383147854, 0.16613672496025436, 0.21502384737678856, 0.14984101748807632, 0.16812400635930047, 0.1836248012718601, 0.1740858505564388, 0.16573926868044514, 0.1907790143084261, 0.17329093799682035, 0.1760731319554849, 0.15779014308426073, 0.17965023847376788, 0.1915739268680445, 0.1705087440381558, 0.1625596184419714, 0.1872019077901431, 0.18044515103338632, 0.19992050874403816, 0.18044515103338632, 0.18322734499205087, 0.1872019077901431, 0.1685214626391097, 0.16971383147853736, 0.16057233704292528, 0.1800476947535771, 0.19554848966613672, 0.15461049284578696, 0.17289348171701113, 0.17170111287758347, 0.1907790143084261, 0.16573926868044514, 0.1800476947535771, 0.1589825119236884, 0.18402225755166932, 0.1538155802861685, 0.19038155802861687, 0.1907790143084261, 0.1856120826709062, 0.18322734499205087, 0.17527821939586646, 0.20786963434022257, 0.17925278219395865])
('val_loss', [3.186827838811757, 3.641355873421664, 3.799027702565318, 3.890430157888675, 3.8670234253751454, 4.1967314055074, 3.9043498800057677, 3.9444634293950798, 4.003241644470415, 4.060317247928326, 4.454848225395801, 4.411667669307446, 4.395950347327276, 4.016937739993128, 3.9350415280233886, 4.438545187120881, 4.787543299254059, 4.688890247932963, 3.882321812994245, 4.43017488285686, 5.171521154491435, 4.429350534995902, 4.96107982665253, 4.645164809493268, 4.685030178123987, 5.31527664924606, 4.985232483554152, 4.125262626747354, 4.8206482190040605, 4.918370903699121, 4.985717138148169, 4.972591653181942, 4.864442457419795, 4.939623911839885, 4.865491834384743, 5.03839023073688, 5.393810726974651, 4.913995231586155, 4.741602978084254, 4.755728975720171, 5.366288830228564, 4.606327534065432, 5.083501515912991, 4.617334579732062, 6.001695158501971, 5.083896125206318, 5.499499206844309, 5.999185562536523, 4.9728542838144945, 5.60669019276984, 5.413963430719554, 5.317698874918, 5.05658218086903, 5.299814149701823, 5.437503516828028, 5.246658285928739, 5.807181837816023, 5.382589548447174, 4.92873177237939, 5.329121530802242, 6.048301873802665, 5.647219494352375, 6.367022269729684, 5.855573970380288, 5.780533228975695, 5.405093830677019, 6.2239427976843285, 6.563487444801437, 4.489810243237095, 6.036677964706868, 6.214566004354453, 5.734976814488254, 5.591648724125565, 6.0199286568264325, 5.932996409615007, 5.621750099279448, 5.838534366179536, 5.671669722171959, 6.046218894943239, 6.112233870193386, 5.585213636212489, 6.137787322728452, 6.825159755214125, 5.163385081935573, 6.0930860424458695, 5.8970466135439885, 5.718139796383051, 6.533090834350388, 6.469604266277368, 6.009282169594105, 6.043933178027218, 5.772686877517423, 5.787005618889779, 6.0629302593975645, 6.1706992324163314, 6.063194526161033, 6.2794044024661915, 6.723540685324677, 5.993621397449595, 6.1081026751423675])
Number of test samples : 44130

44130/44130 [==============================] - 621s 14ms/step

1000 . 2000 . 3000 . 4000 . 5000 . 6000 . 7000 . 8000 . 9000 . 10000 . 11000 . 12000 . 13000 . 14000 . 15000 . 16000 . 17000 . 18000 . 19000 . 20000 . 21000 . 22000 . 23000 . 24000 . 25000 . 26000 . 27000 . 28000 . 29000 . 30000 . 31000 . 32000 . 33000 . 34000 . 35000 . 36000 . 37000 . 38000 . 39000 . 40000 . 41000 . 42000 . 43000 . 44000 . 0 .
Confusion Matrix:
 [[  20   18   60    0    0   13    5   48   43    0    0    0    0   10
     0    0    4    0    0    0    0   21    0    0]
 [  25  111   14    0    1   89   59   25  149   52    0    0   13   15
     0    0    1    0    1    0    0   25    0    0]
 [   5   15   14    1    0    0    9   57   24    1    0    0    6    0
     0    0    0    7    0    0    0    0    0    0]
 [   6    4    0   85  148   13   59  287  126    8    0    0   13    1
     0    0    3  122    0    0    0    0   23    0]
 [   3    8    4    0    0   18   25  144   75   20    0    0   61   19
     0    0    0    0    0    0    0   65    0    0]
 [ 424   31   61    0    0  417  150  160  258   26    1   29  376   36
    39   10    0    6   36    0    0   48    1    0]
 [ 683  254  721   54   18  649 1486 1385 1911  123   33   14  517  208
   103  277  110   43   75    1    0  190  220    1]
 [ 377  119  333   47    3  233  469 1308 1327  130   12   73  149  235
   175   74    7   36   21    0    0  179  113    2]
 [ 389  205  287   75   56   35  568 1043 2089  221    6   17  261  159
   198    3   16   42    8    0    0  182   91    2]
 [  45    2   67   17   11   52  277  640  670  441    0   34   97   88
    64    8   72  296    0    0    0  137   22    0]
 [ 128   24   20    0    0   40  114  205   34    0    0    0    0    8
     1    8    7    3    0    0    0   95    0    0]
 [  33   14  176    0    0   32  367  381  275   55    2   29   85  131
    33    0    5   28   21    0    0   47   11    0]
 [  32   32   78   70    0  199   83  825  638  255    1   20  342   61
    68    1   36   28   56   13    0  110  106    0]
 [  29    3  185   81    6   91  220  553  928   57    6   57  383  155
    42  108   50   90    3    4    0  159  150   13]
 [   7    1    2   15    0    7   16   34   95   17    0    0    8   22
    16   16    3    1    2    0    0   11   12    0]
 [   0    0    0    0    1    3   11   41   27    0    0    0    6    0
     4    0   16   22    0    0    0    6    0    0]
 [   2   44  134    7    0   47   88  485  203   64    0   17    8   61
    29   10   29   76    7    0    0   34   14    0]
 [  57   42  553  196    2  108  377  327  595   35    2   18  252   94
    30    9   13   76    5    0    0  127   63    5]
 [  20   13   64   46    0    6  224  312  130   10    0   36   27   23
    31    1    0   28    2    0    0   89   36    0]
 [   0    0    0   13    4    0   17   17   94    0    0    0    1    0
    18    0    0   20   23    0    0    0   68    0]
 [   0    0    0    0    1    0    2    6    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0]
 [   0    0    8    0    0    5   21    2   17    0    0    0  110    0
     0    0    0    0    0    0    0   17    0    1]
 [   0    1   26   17    1   65  108  107   35   13    0    3    2   31
    58   31   10    4    0    0    0  241   68    0]
 [   3    0    2   27    0    0   26   33  104    2    0    4    0   14
    15    0    1    3    0    0    0    1    4    0]]
/home/devendra/anaconda3/envs/videopipeline/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.
  'precision', 'predicted', average, warn_for)
             precision    recall  f1-score   support

          0       0.01      0.08      0.02       242
          1       0.12      0.19      0.15       580
          2       0.00      0.10      0.01       139
          3       0.11      0.09      0.10       898
          4       0.00      0.00      0.00       442
          5       0.20      0.20      0.20      2109
          6       0.31      0.16      0.21      9076
          7       0.16      0.24      0.19      5422
          8       0.21      0.35      0.26      5953
          9       0.29      0.15      0.19      3040
         10       0.00      0.00      0.00       687
         11       0.08      0.02      0.03      1725
         12       0.13      0.11      0.12      3054
         13       0.11      0.05      0.07      3373
         14       0.02      0.06      0.03       285
         15       0.00      0.00      0.00       137
         16       0.08      0.02      0.03      1359
         17       0.08      0.03      0.04      2986
         18       0.01      0.00      0.00      1098
         19       0.00      0.00      0.00       275
         20       0.00      0.00      0.00         9
         21       0.01      0.09      0.02       181
         22       0.07      0.08      0.07       821
         23       0.00      0.00      0.00       239

avg / total       0.17      0.15      0.15     44130